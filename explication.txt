PROJECT 2 - PACMAN IMITATION LEARNING - Documentation complète


PARTIE 0 : Comment fonctionne un réseau de neurones ? (Pour les vrais débutants)

   Si tu te demandes "mais c'est quoi tous ces trucs de gradients, loss, forward, backward",
   cette section est pour toi. On explique TOUT depuis zéro.

   ÉTAPE 1 : C'est quoi un réseau de neurones ?

   Imagine une grosse fonction mathématique f(x) = y qui prend des inputs x et prédit des outputs y.
   Par exemple :
   - Input x = [position de Pacman, position du ghost, ...]
   - Output y = quelle action choisir (NORTH, SOUTH, EAST, WEST, STOP)

   Cette fonction contient des MILLIONS de nombres appelés "poids" (weights).
   Au début, ces poids sont aléatoires → la fonction fait n'importe quoi.
   L'entraînement = ajuster ces poids pour que la fonction fasse les bonnes prédictions.

   ÉTAPE 2 : Forward Pass (Passage avant)

   C'est le calcul de la prédiction. On fait passer l'input à travers le réseau.

   Exemple concret avec Pacman :
   1. Input : features = [0.25, 0.30, 0.8, ...]  (23 nombres)
   2. Le réseau fait des calculs : Linear → BatchNorm → GELU → Dropout → Linear → ...
   3. Output : logits = [2.1, 0.5, 3.2, 1.0, 0.3]

      Interprétation :
      [2.1, 0.5, 3.2, 1.0, 0.3]
       ↑    ↑    ↑    ↑    ↑
      NORTH SOUTH EAST WEST STOP

   Le réseau prédit EAST car 3.2 est le plus grand nombre.

   ÉTAPE 3 : La Loss (Mesure de l'erreur)

   Problème : Le réseau a prédit EAST, mais l'expert voulait NORTH !

   La loss mesure "à quel point le réseau s'est trompé" :
   - Si le réseau prédit exactement la bonne action → loss = 0 (parfait)
   - Si le réseau se trompe complètement → loss = très grand nombre (nul)

   On utilise CrossEntropyLoss qui calcule :
   Loss = -log(probabilité de la bonne action)

   Dans notre exemple :
   - Logits : [2.1, 0.5, 3.2, 1.0, 0.3]
   - Softmax : [0.18, 0.04, 0.54, 0.06, 0.03]  (conversion en probabilités)
   - Bonne action = NORTH (indice 0), probabilité = 0.18
   - Loss = -log(0.18) = 1.71

   Plus la probabilité est basse, plus la loss est grande.

   ÉTAPE 4 : Backward Pass (Rétropropagation)

   Maintenant qu'on sait que le réseau s'est trompé (loss = 1.71),
   on veut ajuster les poids pour réduire cette erreur.

   Question : Dans quelle direction faut-il bouger chaque poids ?
   Réponse : Les GRADIENTS !

   Le gradient d'un poids = dérivée de la loss par rapport à ce poids.
   Il dit : "Si j'augmente ce poids de 0.01, la loss va augmenter/diminuer de combien ?"

   Exemple :
   - Poids w = 0.5
   - Gradient = -0.3
   - Signification : Si j'augmente w, la loss DIMINUE (c'est bien !)

   loss.backward() calcule automatiquement TOUS les gradients pour TOUS les poids.
   C'est de la magie mathématique (règle de la chaîne en calcul différentiel).

   ÉTAPE 5 : Optimizer (Mise à jour des poids)

   Maintenant qu'on a les gradients, on met à jour les poids.

   FORMULE DE BASE (Gradient Descent classique) :
   nouveau_poids = ancien_poids - learning_rate × gradient

   Exemple :
   - Ancien poids : w = 0.5
   - Gradient : -0.3
   - Learning rate : 0.0008
   - Nouveau poids : w = 0.5 - 0.0008 × (-0.3) = 0.5 + 0.00024 = 0.50024

   PROBLÈME avec Gradient Descent basique :
   Tous les poids utilisent le MÊME learning rate (0.0008).
   Mais certains poids ont besoin de gros changements, d'autres de petits changements !

   SOLUTION : Adam (Adaptive Moment Estimation)

   Adam est INTELLIGENT. Il regarde 2 choses pour adapter le learning rate de chaque poids :

   1. Magnitude du gradient :
      - Gradient FORT (grande pente) → On est loin du minimum → GRANDS pas pour avancer vite
      - Gradient FAIBLE (petite pente) → On est proche du minimum → PETITS pas prudents

   2. Stabilité du gradient (variance) :
      - Gradients STABLES (direction constante) → Adam est CONFIANT → grands pas
      - Gradients INSTABLES (direction change tout le temps) → Adam N'EST PAS CONFIANT → petits pas

   Exemple concret pour un poids w :

   GRADIENTS STABLES (confiant) :
   Itération 1 : gradient = -0.5 (va à gauche)
   Itération 2 : gradient = -0.52 (va à gauche)
   Itération 3 : gradient = -0.48 (va à gauche)
   → Direction claire et constante ! Adam fait de GRANDS pas à gauche

   GRADIENTS INSTABLES (pas confiant) :
   Itération 1 : gradient = +0.5 (va à droite)
   Itération 2 : gradient = -0.3 (va à gauche)
   Itération 3 : gradient = +0.2 (va à droite)
   → Direction qui zigzague ! Adam hésite et fait de PETITS pas prudents

   En gros : Adam fait de GRANDS pas quand il est sûr de la direction, et de PETITS pas quand il hésite.

   Analogie visuelle :
   Imagine que tu descends une montagne dans le brouillard pour trouver la vallée (le minimum) :

   - Gradient FORT = pente raide → tu es loin du fond → tu cours vite (grands pas)
   - Gradient FAIBLE = pente douce → tu es proche du fond → tu marches lentement (petits pas)

   Gradient Descent classique : même vitesse partout
   Adam : adapte la vitesse selon la pente ET la stabilité du terrain

   Techniquement, Adam fait :
   1. Calcule la MOYENNE des gradients récents (momentum)
   2. Calcule la VARIANCE des gradients récents
   3. Adapte le learning rate pour chaque poids individuellement

   Résultat : Adam converge PLUS VITE et de manière PLUS STABLE que Gradient Descent classique.
   C'est pour ça qu'on l'utilise (et presque tout le monde en deep learning l'utilise).

   En résumé :
   - Gradient Descent : learning_rate fixe pour tous les poids
   - Adam : learning_rate adaptatif et intelligent pour chaque poids

   ÉTAPE 6 : Le cycle complet (1 itération d'entraînement)

   1. optim.zero_grad()
      → Efface les anciens gradients (sinon ils s'accumulent)

   2. loss = model.loss(features, actions)
      → Forward pass : calcule la prédiction et la loss

   3. loss.backward()
      → Backward pass : calcule les gradients

   4. optim.step()
      → Met à jour les poids avec les gradients

   On répète ce cycle des MILLIERS de fois (150 epochs × 47 batches = 7050 itérations).
   À chaque itération, les poids s'améliorent un peu.

   ÉTAPE 7 : Pourquoi des epochs et des batches ?

   - 1 batch = groupe de 256 exemples traités ensemble
   - 1 epoch = 1 passage sur TOUT le dataset (15018 exemples)
   - Nombre de batches par epoch = 15018 / 256 = ~47 batches

   Pourquoi des batches ?
   - Traiter 256 exemples ensemble est plus efficace (parallélisation GPU)
   - Le gradient sur 256 exemples est plus stable qu'un seul exemple

   Pourquoi des epochs ?
   - Le réseau a besoin de voir les exemples PLUSIEURS FOIS pour bien apprendre
   - 1 epoch ne suffit pas, il faut 150 epochs pour converger

   ÉTAPE 8 : Résumé visuel du pipeline

   ENTRAÎNEMENT (on modifie les poids) :
   ┌─────────────────────────────────────────────────────┐
   │  for epoch in range(150):                          │
   │    for batch in dataset:                           │
   │                                                     │
   │      1. features, actions = batch                  │
   │         ↓                                           │
   │      2. predictions = model(features)  [FORWARD]   │
   │         ↓                                           │
   │      3. loss = compare(predictions, actions)       │
   │         ↓                                           │
   │      4. loss.backward()  [BACKWARD → gradients]    │
   │         ↓                                           │
   │      5. optim.step()  [Update weights]             │
   │         ↓                                           │
   │      Les poids sont maintenant meilleurs !         │
   └─────────────────────────────────────────────────────┘

   ÉVALUATION (on teste, sans modifier les poids) :
   ┌─────────────────────────────────────────────────────┐
   │  with torch.no_grad():  [pas de gradients]         │
   │    for batch in test_dataset:                      │
   │                                                     │
   │      1. features, actions = batch                  │
   │         ↓                                           │
   │      2. predictions = model(features)  [FORWARD]   │
   │         ↓                                           │
   │      3. accuracy = compare(predictions, actions)   │
   │         ↓                                           │
   │      On mesure la performance, c'est tout !        │
   └─────────────────────────────────────────────────────┘

   Différence clé :
   - ENTRAÎNEMENT 80%: Forward → Loss → Backward → Update weights
   - ÉVALUATION 20%: Forward → Accuracy (pas de backward, pas d'update)

   ÉTAPE 9 : Pourquoi model.train() et model.eval() ?

   model.train() :
   - Active le dropout : désactive aléatoirement 30% des neurones
   - Pourquoi ? Pour éviter l'overfitting (le réseau mémorise au lieu d'apprendre)

   model.eval() :
   - Désactive le dropout : utilise TOUS les neurones
   - Pourquoi ? Pour avoir des prédictions stables et déterministes

   ÉTAPE 10 : Analogie finale

   Imagine que tu apprends à jouer au basket :
   - Forward pass = tu tires au panier
   - Loss = distance entre le ballon et le panier (ton erreur)
   - Gradients = "j'aurais dû tirer plus fort" ou "plus à gauche"
   - Optimizer = tu ajustes ta technique pour le prochain tir
   - Epochs = tu t'entraînes pendant des semaines

   Au début tu rates beaucoup (loss élevée).
   Après 150 jours d'entraînement, tu réussis 87% de tes tirs (accuracy = 87%).

   C'est exactement pareil pour le réseau de neurones !

   Maintenant quand tu vois ce code :
      loss = model.loss(features, actions)
      optim.zero_grad()
      loss.backward()
      optim.step()

   Tu comprends : "Ah oui, il calcule l'erreur, calcule les gradients,
   et ajuste les poids pour faire mieux la prochaine fois !"


PARTIE 1 : Vue d'ensemble

   But: apprendre à Pacman à imiter un expert en utilisant l'apprentissage supervisé.
   Depuis un dataset de paires (GameState, action)
   et on veut entraîner un réseau de neurones à prédire quelle action l'expert aurait pris.

   Ordre de lecture : ici les 4 fichiers qu'on a modifié
   1. data.py - Convertit GameState en 23 features
   2. architecture.py - Définit le réseau MLP (23 → 256 → 128 → 64 → 5)
   3. train.py - Entraîne le modèle (simple console output)
   4. pacmanagent.py - Utilise le modèle pour jouer


   Quick summary de chaque fichier :

   data.py (Feature Engineering)
   La partie la plus importante. On transforme le GameState en 23 features :
   position, distances Manhattan pour ghost/food, géométrie du labyrinthe,
   actions légales, et niveau de danger. Tout est normalisé pour que le réseau converge bien.

   architecture.py (Réseau de neurones)
   MLP simple : 23 → 256 → 128 → 64 → 5 (3 couches cachées)
   Pattern par couche : Linear → BatchNorm → GELU → Dropout(0.3)
   Architecture similaire aux MLPs classiques pour la classification (voir MNIST [5]).

   train.py (Entraînement)
   Split train/test 80/20, Adam optimizer, 150 epochs, batch_size=256.
   Utilise CrossEntropyLoss pour classification multi-classe (5 actions).
   Affiche l'accuracy à chaque epoch et sauvegarde le meilleur modèle.

   pacmanagent.py (Agent)
   Utilise le modèle entraîné pour jouer.
   Convertit l'état → features → forward → softmax → meilleure action légale.


   Structure du projet :
   project2/
   ├── datasets/
   │   ├── pacman_dataset.pkl    → 15018 paires (state, action) de l'expert
   │   └── pacman_test.pkl       → États de jeu sans actions pour Gradescope
   ├── pacman_module/            → Moteur de jeu (NE PAS MODIFIER)
   ├── data.py                   → Feature engineering + Dataset
   ├── architecture.py           → Réseau de neurones (MLP)
   ├── train.py                  → Boucle d'entraînement + visualisation intégrée
   ├── pacmanagent.py            → Agent qui joue
   ├── run.py                    → Visualisation du jeu
   ├── write_submission.py       → Génération du CSV
   ├── pacman_model.pth          → Poids du modèle entraîné
   └── submission.csv            → Prédictions pour Gradescope


PARTIE 2 : Data.py - Feature Engineering

   Un réseau de neurones ne peut pas apprendre directement d'un GameState brut.
   On doit extraire des informations pertinentes pour la prise de décision.

   Nos 23 features (toutes normalisées ~[0,1]):

   POSITION PACMAN (2 features):
   [0] px / 20       Position X de Pacman
   [1] py / 20       Position Y de Pacman

   INFORMATION GHOST (4 features):
   [2] direction_to_ghost_x    Direction vers le ghost (normalisée)
   [3] direction_to_ghost_y    Direction vers le ghost (normalisée)
   [4] ghost_mantt_dist / diam Distance Manhattan au ghost
   [5] ghost_adjacent          1 si ghost adjacent (distance ≤ 1)

   INFORMATION FOOD (4 features):
   [6] n_food / 50                  Nombre de food restante
   [7] direction_to_food_x          Direction vers la food la plus proche
   [8] direction_to_food_y          Direction vers la food la plus proche
   [9] closest_food_dist / diam     Distance Manhattan à la food la plus proche

   GEOMETRIE DU LABYRINTHE (5 features):
   [10] dist_north / H  Distance au mur au Nord
   [11] dist_south / H  Distance au mur au Sud
   [12] dist_east  / W  Distance au mur à l'Est
   [13] dist_west  / W  Distance au mur à l'Ouest
   [14] is_corner       1 si c'est un coin (≤2 directions légales)

   DANGER (3 features):
   [15] danger_level      1.0 / max(ghost_dist, 0.5) - Plus élevé = plus dangereux
   [16] ghost_blocks_food 1 si ghost est entre Pacman et la food
   [17] escape_options    Nombre de directions légales (hors STOP) / 4

   ACTIONS LEGALES (5 features):
   [18] legal_north  1 si NORTH est légal
   [19] legal_south  1 si SOUTH est légal
   [20] legal_east   1 si EAST est légal
   [21] legal_west   1 si WEST est légal
   [22] legal_stop   1 si STOP est légal


   Comment on trouve la food la plus proche ?

   1. On récupère toutes les positions de food
      food_positions = [(5, 3), (7, 8), (2, 4), ...]

   2. On calcule la distance Manhattan à CHAQUE food
      distances = [abs(pac_x - food_x) + abs(pac_y - food_y)
                  for food_x, food_y in food_positions]

   3. On trouve laquelle est la plus proche
      closest_index = argmin(distances)

   4. On calcule la direction vers cette food
      direction_x = closest_food_x - pac_x
      direction_y = closest_food_y - pac_y

      Si direction_x > 0 → food à droite
      Si direction_x < 0 → food à gauche
      Si direction_y > 0 → food en haut
      Si direction_y < 0 → food en bas


   Géométrie du labyrinthe

   La fonction dist_until_wall() compte combien de cases Pacman peut avancer
   dans une direction avant de rencontrer un mur.

   Exemple visuel:
      # # # # # # #
      # . . . . . #
      # # P . . . #  ← Pacman en (2,2)
      # # # . . . #
      # # # # # # #

   dist_until_wall(2, 2, 1, 0) = 3  (peut aller 3 cases à droite)
   dist_until_wall(2, 2, -1, 0) = 0  (mur immédiat à gauche)

   Détection de coin :
   On compte combien de directions sont libres autour de Pacman.
   Si ≤2, c'est un coin (dangereux car peu d'options pour fuir).


PARTIE 3 : Architecture (architecture.py)

   Pourquoi un MLP et pas un CNN ?
   Les CNN sont faits pour les images 2D où les pixels voisins sont liés (ex: MNIST [5]).
   Notre input est un vecteur (je dirais même un tenseur!) 1D de 23 features sans relation spatiale,
   donc un MLP est le bon choix - similaire aux architectures fully-connected classiques.

   Structure : 23 → 256 → 128 → 64 → 5

   Pour chaque couche cachée ici 3: Linear → BatchNorm → GELU → Dropout

   1. Linear : transforme les données (multiplication matricielle + biais)
   2. BatchNorm [1] : normalise les valeurs entre couches (stabilise l'entraînement)
   3. GELU [2] : activation smooth, meilleure que ReLU
   4. Dropout [3] : prévient l'overfitting (p=0.3)
   5. Pas d'activation finale : CrossEntropyLoss veut des logits bruts

   Pourquoi GELU et pas ReLU ?
   GELU [2] est plus smooth (pas de coupure brutale à 0),
   meilleur flow de gradients, utilisé dans GPT et BERT.


PARTIE 4 : Entraînement (train.py)

   Pipeline :
   1. Charger le dataset (15018 échantillons)
   2. Split train/test (80%/20%) avec random_split
   3. Entraîner pendant 150 epochs
   4. Évaluer sur test set et sauvegarder le modèle

   Hyperparamètres optimisés :
   - batch_size = 256      → Batch size élevé pour gradients stables
   - epochs = 150          → Assez d'epochs pour convergence complète
   - lr = 8e-4             → Learning rate légèrement élevé pour converger plus vite
   - test_ratio = 0.2      → 20% pour évaluation finale

   Modifications importantes :
   - Affichage simple de l'accuracy à chaque epoch
   - Évaluation du test set à chaque epoch
   - Checkpointing : sauvegarde automatique du meilleur modèle basé sur test accuracy


   Loss function : CrossEntropyLoss

   La loss mesure à quel point le réseau se trompe (0 = parfait, plus c'est grand, plus il est loin de la bonne action).
   Le gradient, c'est la dérivée de cette loss par rapport aux poids : il indique dans quelle direction et de combien on doit modifier les poids pour réduire l'erreur (descente de gradient).

   Vu au cours pour les problèmes de classification.
   Mais pourquoi on a besoin d'une "loss" ? Explications depuis le début :

            ÉTAPE 1 : Qu'est-ce qu'on veut faire ?

            On veut que le réseau apprenne à imiter l'expert.
            Pour chaque situation de jeu, l'expert a choisi une action (ex: EAST).
            On veut que le réseau apprenne à faire le même choix.

            Exemple concret :
            - État du jeu : Pacman en (5,3), ghost à droite, food en haut
            - Expert a choisi : NORTH (indice 0)
            - Réseau prédit : [2.1, 0.5, 3.2, 1.0, 0.3]  ← nombres bruts (logits)
                              ↑    ↑    ↑    ↑    ↑
                              NORTH SOUTH EAST WEST STOP

            Problème : Le réseau prédit 3.2 pour EAST, mais l'expert voulait NORTH !
            On doit dire au réseau "tu t'es trompé, il fallait choisir NORTH".

            ÉTAPE 2 : Comment mesurer l'erreur ?

            D'abord, convertir les logits en probabilités avec SOFTMAX :

            Softmax(x_i) = exp(x_i) / Σ exp(x_j)

            Logits :        [2.1,  0.5,  3.2,  1.0,  0.3]
                              ↓     ↓     ↓     ↓     ↓
            Probabilités :  [0.18, 0.04, 0.54, 0.06, 0.03]
                              ↑
                           NORTH = 18% seulement (mais c'est la bonne action!)

            Les probabilités somment à 1.0 (= 100%).
            On voit que le réseau donne seulement 18% de confiance à NORTH,
            alors que c'est l'action correcte !

            ÉTAPE 3 : Calculer la perte (loss)

            La loss mesure "à quel point le réseau s'est trompé".

            On veut pénaliser le réseau quand :
            - La probabilité de la BONNE action est FAIBLE (mauvais !)
            - La probabilité de la BONNE action est ÉLEVÉE (bon !)

            Formule mathématique (pour 1 exemple) :
               Loss = -log(probabilité_de_la_bonne_action)

            Dans notre exemple :
               Bonne action = NORTH, probabilité = 0.18
               Loss = -log(0.18) = 1.71

            Plus la probabilité est basse, plus la loss est élevée (= grosse pénalité).
            Si probabilité = 1.0 (100% sûr), alors loss = 0 (parfait !).

            ÉTAPE 4 : Cross Entropy = moyenne sur tout le batch

            Un batch = groupe de 256 exemples traités ensemble.
            On calcule la loss pour CHAQUE exemple, puis on fait la moyenne.

            Formule complète (du cours) :
               L(θ) = -1/N Σ_(x_j,y_j)∈d Σ_{i=1}^C y_ij log f_i(x_j; θ)

            Où :
            - N = nombre d'exemples dans le batch (256)
            - C = nombre de classes (5 actions)
            - y_ij = 1 si l'exemple j appartient à la classe i, 0 sinon
            - f_i(x_j; θ) = probabilité prédite par le réseau pour la classe i

            En clair : Pour chaque exemple, prends -log(proba de la bonne action),
            puis fais la moyenne sur tout le batch.

            ÉTAPE 5 : Pourquoi CrossEntropyLoss en PyTorch ?

            Manuellement, il faudrait faire :
            1. Logits → Softmax → probabilités
            2. Prendre le log des probabilités → log-probabilités
            3. Sélectionner la bonne action et calculer -log(proba)
            4. Faire la moyenne

            PyTorch fait TOUT ça automatiquement avec CrossEntropyLoss() !

            Bonus : PyTorch utilise LogSoftmax au lieu de Softmax + Log séparément.
            Pourquoi ? Pour la stabilité numérique (éviter les overflows avec exp()).

            Résumé :
               CrossEntropyLoss = Softmax + Log + Sélection + Moyenne

            En une seule ligne de code, PyTorch :
            - Convertit les logits en probabilités
            - Calcule -log(probabilité de la bonne action)
            - Fait la moyenne sur le batch
            - Tout ça de manière stable et rapide !

            C'est pour ça qu'on n'a PAS besoin de mettre Softmax à la fin du réseau :
            CrossEntropyLoss s'attend à recevoir des logits bruts et fait le reste.

   Optimizer : Adam [4]
   Adam est le choix par défaut pour le deep learning car il adapte
   le learning rate par paramètre et converge rapidement.
   Fonctionne bien avec notre architecture inspirée des MLPs classiques (MNIST [5]).

      Checkpointing - Sauvegarde du meilleur modèle :
      À chaque epoch, on évalue l'accuracy sur le test set.
      Si l'accuracy actuelle > meilleure accuracy précédente, on sauvegarde le modèle.
      Cela garantit qu'on garde le modèle qui généralise le mieux, pas le dernier
      (qui pourrait avoir overfitté). Le modèle est sauvegardé dans pacman_model.pth.


PARTIE 5 : Agent (pacmanagent.py)

   Juste lire le .py suffit


PARTIE 6 : Questions possibles à l'oral - FAQ

   Q: Pourquoi un MLP et pas un CNN?
   R: Les CNN sont faits pour les images 2D où les pixels voisins sont liés.
   Notre input est un vecteur 1D de 23 features sans relation spatiale,
   donc un MLP est le bon choix.

   Q: Pourquoi normaliser les features?
   R: Les NN convergent mieux quand les inputs sont dans la même échelle.
   Sans normalisation, certaines features domineraient les autres.

   Q: Pourquoi GELU?
   R: Activation smooth avec meilleur flow de gradients que ReLU.
   Utilisé dans GPT, BERT. Performance empiriquement supérieure.

   Q: Pourquoi BatchNorm?
   R: Sans BatchNorm [1], l'entraînement était instable.
   BatchNorm normalise les valeurs entre couches pour stabiliser les gradients.
   Permet d'utiliser un learning rate plus élevé et convergence plus rapide.

   Q: Pourquoi CrossEntropyLoss?
   R: C'est la loss standard pour la classification multi-classe.
   Combine LogSoftmax et NLLLoss efficacement. Voir cours théorique.

   Q: Pourquoi les features de danger?
   R: Le modèle avait du mal à éviter les ghosts avec juste la distance.
   danger_level donne un gradient de danger,
   ghost_blocks_food dit si le chemin vers la food est dangereux,
   escape_options dit combien de fuites sont possibles.
   C'est pr moi les features les + importantes.

   Q: Comment gérer les actions illégales?
   R: On trie les prédictions par probabilité et on prend la première action
   qui est dans la liste des actions légales.

   Q: Pourquoi 150 epochs avec batch_size=256 et lr=8e-4?
   R: Optimisé empiriquement. Batch size plus grand = gradients plus stables.
   150 epochs permettent une convergence complète (best model à epoch ~121).
   LR légèrement élevé (8e-4 au lieu de 5e-4) accélère la convergence.

   Q: Pourquoi dropout=0.3?
   R: Balance entre regularisation et capacité du réseau.
   Trop de dropout (0.4) = underfitting, trop peu (0.15) = risque d'overfitting.
   0.3 est le sweet spot pour notre architecture.

   Q: Pourquoi normaliser ?
   R: Les réseaux de neurones fonctionnent mieux quand toutes les features
   sont dans la même échelle.
   Sans normalisation, la position X pourrait être 0-20,
   le score pourrait être -500 à +1000, les flags sont 0 ou 1...
   Avec normalisation tout est ~[0,1], ce qui accélère la convergence
   et évite que certaines features dominent les autres.

   Q: Comment la normalisation s'adapte aux différents mazes ?
   R: On utilise les dimensions réelles du maze pour normaliser :
   - Positions : pac_x / maze_width, pac_y / maze_height
   - Distances : distances / (maze_width + maze_height)
   - Murs : dist_north / maze_height, dist_east / maze_width
   Comme ça, peu importe la taille du maze (15x15 ou 25x25),
   les features restent dans [0, 1] et le modèle généralise bien.

   Q: Pourquoi convertir les actions en indices ?
   R: Les réseaux de neurones ne comprennent que les nombres,
   pas les objets Python comme Directions.NORTH.
   On crée donc deux mappings :

   ACTION_TO_INDEX = {
      Directions.NORTH: 0,
      Directions.SOUTH: 1,
      Directions.EAST: 2,
      Directions.WEST: 3,
      Directions.STOP: 4
   }

   INDEX_TO_ACTION = {0: Directions.NORTH, ...}

   Pendant l'entraînement :
   Expert joue EAST → converti en indice 2 → Le réseau apprend à prédire 2

   Pendant le jeu :
   Réseau prédit [0.05, 0.1, 0.75, 0.08, 0.02] → argmax = 2 → reconverti en EAST

   Q: Pourquoi l'accuracy plafonne à ~88-89% ?
   R: L'accuracy mesure si le réseau prédit EXACTEMENT la même action que l'expert.
   Mais dans ~10-15% des situations, il y a plusieurs actions valides (ex: ghost loin,
   plusieurs chemins vers la food). Le réseau peut choisir une action différente mais
   tout aussi bonne. C'est normal et ça ne veut pas dire que le modèle est mauvais.

   Q: Est-ce que le modèle va fonctionner sur Gradescope avec des mazes différents ?
   R: Oui ! Les features sont maze-aware :
   - legal_actions s'adapte automatiquement aux murs
   - dist_until_wall détecte les murs dans chaque direction
   - is_corner s'adapte à la topologie du maze
   Le modèle a appris des patterns généraux (fuir les ghosts, aller vers la food),
   pas juste à mémoriser un maze spécifique.
