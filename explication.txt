PROJECT 2 - PACMAN IMITATION LEARNING - Documentation complète


PARTIE 1 : Vue d'ensemble

   But: apprendre à Pacman à imiter un expert en utilisant l'apprentissage supervisé.
   Depuis un dataset de paires (GameState, action)
   et on veut entraîner un réseau de neurones à prédire quelle action l'expert aurait pris.

   Ordre de lecture : ici les 4 fichiers qu'on a modifié
   1. data.py - Convertit GameState en 23 features
   2. architecture.py - Définit le réseau MLP (23 → 256 → 128 → 64 → 5)
   3. train.py - Entraîne le modèle (avec fonction visualization() pour les graphiques)
   4. pacmanagent.py - Utilise le modèle pour jouer


   Quick summary de chaque fichier :

   data.py (Feature Engineering)
   La partie la plus importante. On transforme le GameState en 23 features :
   position, distances Manhattan pour ghost/food, géométrie du labyrinthe,
   actions légales, et niveau de danger. Tout est normalisé pour que le réseau converge bien.

   architecture.py (Réseau de neurones)
   MLP simple : 23 → 256 → 128 → 64 → 5 (3 couches cachées)
   Pattern par couche : Linear → BatchNorm → GELU → Dropout(0.3)
   Architecture similaire aux MLPs classiques pour la classification (voir MNIST [5]).

   train.py (Entraînement)
   Split train/test 80/20, Adam optimizer, 150 epochs, batch_size=256.
   Utilise CrossEntropyLoss pour classification multi-classe (5 actions).
   Évaluation finale sur le test set avec calcul de l'accuracy.
   Code modulaire : fonction visualization() intégrée pour les graphiques.

   pacmanagent.py (Agent)
   Utilise le modèle entraîné pour jouer.
   Convertit l'état → features → forward → softmax → meilleure action légale.


   Structure du projet :
   project2/
   ├── datasets/
   │   ├── pacman_dataset.pkl    → 15018 paires (state, action) de l'expert
   │   └── pacman_test.pkl       → États de jeu sans actions pour Gradescope
   ├── pacman_module/            → Moteur de jeu (NE PAS MODIFIER)
   ├── data.py                   → Feature engineering + Dataset
   ├── architecture.py           → Réseau de neurones (MLP)
   ├── train.py                  → Boucle d'entraînement + visualisation intégrée
   ├── pacmanagent.py            → Agent qui joue
   ├── run.py                    → Visualisation du jeu
   ├── write_submission.py       → Génération du CSV
   ├── pacman_model.pth          → Poids du modèle entraîné
   └── submission.csv            → Prédictions pour Gradescope


PARTIE 2 : Data.py - Feature Engineering

   Un réseau de neurones ne peut pas apprendre directement d'un GameState brut.
   On doit extraire des informations pertinentes pour la prise de décision.

   Nos 23 features (toutes normalisées ~[0,1]):

   POSITION PACMAN (2 features):
   [0] px / 20       Position X de Pacman
   [1] py / 20       Position Y de Pacman

   INFORMATION GHOST (4 features):
   [2] direction_to_ghost_x    Direction vers le ghost (normalisée)
   [3] direction_to_ghost_y    Direction vers le ghost (normalisée)
   [4] ghost_mantt_dist / diam Distance Manhattan au ghost
   [5] ghost_adjacent          1 si ghost adjacent (distance ≤ 1)

   INFORMATION FOOD (4 features):
   [6] n_food / 50                  Nombre de food restante
   [7] direction_to_food_x          Direction vers la food la plus proche
   [8] direction_to_food_y          Direction vers la food la plus proche
   [9] closest_food_dist / diam     Distance Manhattan à la food la plus proche

   GEOMETRIE DU LABYRINTHE (5 features):
   [10] dist_north / H  Distance au mur au Nord
   [11] dist_south / H  Distance au mur au Sud
   [12] dist_east  / W  Distance au mur à l'Est
   [13] dist_west  / W  Distance au mur à l'Ouest
   [14] is_corner       1 si c'est un coin (≤2 directions légales)

   DANGER (3 features):
   [15] danger_level      1.0 / max(ghost_dist, 0.5) - Plus élevé = plus dangereux
   [16] ghost_blocks_food 1 si ghost est entre Pacman et la food
   [17] escape_options    Nombre de directions légales (hors STOP) / 4

   ACTIONS LEGALES (5 features):
   [18] legal_north  1 si NORTH est légal
   [19] legal_south  1 si SOUTH est légal
   [20] legal_east   1 si EAST est légal
   [21] legal_west   1 si WEST est légal
   [22] legal_stop   1 si STOP est légal


   Comment on trouve la food la plus proche ?

   1. On récupère toutes les positions de food
      food_positions = [(5, 3), (7, 8), (2, 4), ...]

   2. On calcule la distance Manhattan à CHAQUE food
      distances = [abs(pac_x - food_x) + abs(pac_y - food_y)
                  for food_x, food_y in food_positions]

   3. On trouve laquelle est la plus proche
      closest_index = argmin(distances)

   4. On calcule la direction vers cette food
      direction_x = closest_food_x - pac_x
      direction_y = closest_food_y - pac_y

      Si direction_x > 0 → food à droite
      Si direction_x < 0 → food à gauche
      Si direction_y > 0 → food en haut
      Si direction_y < 0 → food en bas


   Géométrie du labyrinthe

   La fonction dist_until_wall() compte combien de cases Pacman peut avancer
   dans une direction avant de rencontrer un mur.

   Exemple visuel:
      # # # # # # #
      # . . . . . #
      # # P . . . #  ← Pacman en (2,2)
      # # # . . . #
      # # # # # # #

   dist_until_wall(2, 2, 1, 0) = 3  (peut aller 3 cases à droite)
   dist_until_wall(2, 2, -1, 0) = 0  (mur immédiat à gauche)

   Détection de coin :
   On compte combien de directions sont libres autour de Pacman.
   Si ≤2, c'est un coin (dangereux car peu d'options pour fuir).


PARTIE 3 : Architecture (architecture.py)

   Pourquoi un MLP et pas un CNN ?
   Les CNN sont faits pour les images 2D où les pixels voisins sont liés (ex: MNIST [5]).
   Notre input est un vecteur (je dirais même un tenseur!) 1D de 23 features sans relation spatiale,
   donc un MLP est le bon choix - similaire aux architectures fully-connected classiques.

   Structure : 23 → 256 → 128 → 64 → 5

   Pour chaque couche cachée ici 3: Linear → BatchNorm → GELU → Dropout

   1. Linear : transforme les données (multiplication matricielle + biais)
   2. BatchNorm [1] : normalise les valeurs entre couches (stabilise l'entraînement)
   3. GELU [2] : activation smooth, meilleure que ReLU
   4. Dropout [3] : prévient l'overfitting (p=0.3)
   5. Pas d'activation finale : CrossEntropyLoss veut des logits bruts

   Pourquoi GELU et pas ReLU ?
   GELU [2] est plus smooth (pas de coupure brutale à 0),
   meilleur flow de gradients, utilisé dans GPT et BERT.


PARTIE 4 : Entraînement (train.py)

   Pipeline :
   1. Charger le dataset (15018 échantillons)
   2. Split train/test (80%/20%) avec random_split
   3. Entraîner pendant 150 epochs avec visualisation
   4. Évaluer sur test set et sauvegarder le modèle

   Hyperparamètres optimisés :
   - batch_size = 256      → Batch size élevé pour gradients stables
   - epochs = 150          → Assez d'epochs pour convergence complète
   - lr = 8e-4             → Learning rate légèrement élevé pour converger plus vite
   - test_ratio = 0.2      → 20% pour évaluation finale
   - show_every = 30       → Fréquence de visualisation des plots

   Modifications importantes :
   - Utilisation de tqdm/trange pour progression en temps réel
   - Visualisation interactive de la loss + accuracy pendant l'entraînement
   - Évaluation du test set à chaque epoch
   - Checkpointing : sauvegarde automatique du meilleur modèle basé sur test accuracy

   Visualisation (fonction visualization()) :
   Fonction intégrée qui retourne deux fonctions pour gérer les graphiques :

   1. setup_plots(num_plots)
      - Crée une fenêtre unique avec grille de subplots
      - Loss plots à gauche (Epoch 0, 30, 60, 90, 120, Final)
      - Accuracy plot à droite (courbe continue)
      - Retourne (fig, axes, ax_acc)

   2. update_plots(axes, ax_acc, plot_idx, losses, ...)
      - Met à jour les graphiques en temps réel
      - Gère les mises à jour périodiques (toutes les 30 epochs)
      - Gère la mise à jour finale (is_final=True)
      - Refresh automatique avec plt.pause()


   Loss function : CrossEntropyLoss

   La loss mesure à quel point le réseau se trompe (0 = parfait, plus c'est grand, plus il est loin de la bonne action).
   Le gradient, c'est la dérivée de cette loss par rapport aux poids : il indique dans quelle direction et de combien on doit modifier les poids pour réduire l'erreur (descente de gradient).

   Vu au cours pour les problèmes de classification.
   Mais pourquoi on a besoin d'une "loss" ? Explications depuis le début :

            ÉTAPE 1 : Qu'est-ce qu'on veut faire ?

            On veut que le réseau apprenne à imiter l'expert.
            Pour chaque situation de jeu, l'expert a choisi une action (ex: EAST).
            On veut que le réseau apprenne à faire le même choix.

            Exemple concret :
            - État du jeu : Pacman en (5,3), ghost à droite, food en haut
            - Expert a choisi : NORTH (indice 0)
            - Réseau prédit : [2.1, 0.5, 3.2, 1.0, 0.3]  ← nombres bruts (logits)
                              ↑    ↑    ↑    ↑    ↑
                              NORTH SOUTH EAST WEST STOP

            Problème : Le réseau prédit 3.2 pour EAST, mais l'expert voulait NORTH !
            On doit dire au réseau "tu t'es trompé, il fallait choisir NORTH".

            ÉTAPE 2 : Comment mesurer l'erreur ?

            D'abord, convertir les logits en probabilités avec SOFTMAX :

            Softmax(x_i) = exp(x_i) / Σ exp(x_j)

            Logits :        [2.1,  0.5,  3.2,  1.0,  0.3]
                              ↓     ↓     ↓     ↓     ↓
            Probabilités :  [0.18, 0.04, 0.54, 0.06, 0.03]
                              ↑
                           NORTH = 18% seulement (mais c'est la bonne action!)

            Les probabilités somment à 1.0 (= 100%).
            On voit que le réseau donne seulement 18% de confiance à NORTH,
            alors que c'est l'action correcte !

            ÉTAPE 3 : Calculer la perte (loss)

            La loss mesure "à quel point le réseau s'est trompé".

            On veut pénaliser le réseau quand :
            - La probabilité de la BONNE action est FAIBLE (mauvais !)
            - La probabilité de la BONNE action est ÉLEVÉE (bon !)

            Formule mathématique (pour 1 exemple) :
               Loss = -log(probabilité_de_la_bonne_action)

            Dans notre exemple :
               Bonne action = NORTH, probabilité = 0.18
               Loss = -log(0.18) = 1.71

            Plus la probabilité est basse, plus la loss est élevée (= grosse pénalité).
            Si probabilité = 1.0 (100% sûr), alors loss = 0 (parfait !).

            ÉTAPE 4 : Cross Entropy = moyenne sur tout le batch

            Un batch = groupe de 256 exemples traités ensemble.
            On calcule la loss pour CHAQUE exemple, puis on fait la moyenne.

            Formule complète (du cours) :
               L(θ) = -1/N Σ_(x_j,y_j)∈d Σ_{i=1}^C y_ij log f_i(x_j; θ)

            Où :
            - N = nombre d'exemples dans le batch (256)
            - C = nombre de classes (5 actions)
            - y_ij = 1 si l'exemple j appartient à la classe i, 0 sinon
            - f_i(x_j; θ) = probabilité prédite par le réseau pour la classe i

            En clair : Pour chaque exemple, prends -log(proba de la bonne action),
            puis fais la moyenne sur tout le batch.

            ÉTAPE 5 : Pourquoi CrossEntropyLoss en PyTorch ?

            Manuellement, il faudrait faire :
            1. Logits → Softmax → probabilités
            2. Prendre le log des probabilités → log-probabilités
            3. Sélectionner la bonne action et calculer -log(proba)
            4. Faire la moyenne

            PyTorch fait TOUT ça automatiquement avec CrossEntropyLoss() !

            Bonus : PyTorch utilise LogSoftmax au lieu de Softmax + Log séparément.
            Pourquoi ? Pour la stabilité numérique (éviter les overflows avec exp()).

            Résumé :
               CrossEntropyLoss = Softmax + Log + Sélection + Moyenne

            En une seule ligne de code, PyTorch :
            - Convertit les logits en probabilités
            - Calcule -log(probabilité de la bonne action)
            - Fait la moyenne sur le batch
            - Tout ça de manière stable et rapide !

            C'est pour ça qu'on n'a PAS besoin de mettre Softmax à la fin du réseau :
            CrossEntropyLoss s'attend à recevoir des logits bruts et fait le reste.

   Optimizer : Adam [4]
   Adam est le choix par défaut pour le deep learning car il adapte
   le learning rate par paramètre et converge rapidement.
   Fonctionne bien avec notre architecture inspirée des MLPs classiques (MNIST [5]).

      Checkpointing - Sauvegarde du meilleur modèle :
      À chaque epoch, on évalue l'accuracy sur le test set.
      Si l'accuracy actuelle > meilleure accuracy précédente, on sauvegarde le modèle.
      Cela garantit qu'on garde le modèle qui généralise le mieux, pas le dernier
      (qui pourrait avoir overfitté). Le modèle est sauvegardé dans pacman_model.pth.


PARTIE 5 : Agent (pacmanagent.py)

   Juste lire le .py suffit


PARTIE 6 : Questions possibles à l'oral - FAQ

   Q: Pourquoi un MLP et pas un CNN?
   R: Les CNN sont faits pour les images 2D où les pixels voisins sont liés.
   Notre input est un vecteur 1D de 23 features sans relation spatiale,
   donc un MLP est le bon choix.

   Q: Pourquoi normaliser les features?
   R: Les NN convergent mieux quand les inputs sont dans la même échelle.
   Sans normalisation, certaines features domineraient les autres.

   Q: Pourquoi GELU?
   R: Activation smooth avec meilleur flow de gradients que ReLU.
   Utilisé dans GPT, BERT. Performance empiriquement supérieure.

   Q: Pourquoi BatchNorm?
   R: Sans BatchNorm [1], l'entraînement était instable.
   BatchNorm normalise les valeurs entre couches pour stabiliser les gradients.
   Permet d'utiliser un learning rate plus élevé et convergence plus rapide.

   Q: Pourquoi CrossEntropyLoss?
   R: C'est la loss standard pour la classification multi-classe.
   Combine LogSoftmax et NLLLoss efficacement.

   Q: Pourquoi les features de danger?
   R: Le modèle avait du mal à éviter les ghosts avec juste la distance.
   danger_level donne un gradient de danger,
   ghost_blocks_food dit si le chemin vers la food est dangereux,
   escape_options dit combien de fuites sont possibles.

   Q: Comment gérer les actions illégales?
   R: On trie les prédictions par probabilité et on prend la première action
   qui est dans la liste des actions légales.

   Q: Pourquoi 150 epochs avec batch_size=256 et lr=8e-4?
   R: Optimisé empiriquement. Batch size plus grand = gradients plus stables.
   150 epochs permettent une convergence complète (best model à epoch ~121).
   LR légèrement élevé (8e-4 au lieu de 5e-4) accélère la convergence.

   Q: Pourquoi dropout=0.3?
   R: Balance entre regularisation et capacité du réseau.
   Trop de dropout (0.4) = underfitting, trop peu (0.15) = risque d'overfitting.
   0.3 est le sweet spot pour notre architecture.

   Q: Pourquoi normaliser ?
   R: Les réseaux de neurones fonctionnent mieux quand toutes les features
   sont dans la même échelle.
   Sans normalisation, la position X pourrait être 0-20,
   le score pourrait être -500 à +1000, les flags sont 0 ou 1...
   Avec normalisation tout est ~[0,1], ce qui accélère la convergence
   et évite que certaines features dominent les autres.

   Q: Comment la normalisation s'adapte aux différents mazes ?
   R: On utilise les dimensions réelles du maze pour normaliser :
   - Positions : pac_x / maze_width, pac_y / maze_height
   - Distances : distances / (maze_width + maze_height)
   - Murs : dist_north / maze_height, dist_east / maze_width
   Comme ça, peu importe la taille du maze (15x15 ou 25x25),
   les features restent dans [0, 1] et le modèle généralise bien.

   Q: Pourquoi convertir les actions en indices ?
   R: Les réseaux de neurones ne comprennent que les nombres,
   pas les objets Python comme Directions.NORTH.
   On crée donc deux mappings :

   ACTION_TO_INDEX = {
      Directions.NORTH: 0,
      Directions.SOUTH: 1,
      Directions.EAST: 2,
      Directions.WEST: 3,
      Directions.STOP: 4
   }

   INDEX_TO_ACTION = {0: Directions.NORTH, ...}

   Pendant l'entraînement :
   Expert joue EAST → converti en indice 2 → Le réseau apprend à prédire 2

   Pendant le jeu :
   Réseau prédit [0.05, 0.1, 0.75, 0.08, 0.02] → argmax = 2 → reconverti en EAST

   Q: Pourquoi l'accuracy plafonne à ~88-89% ?
   R: L'accuracy mesure si le réseau prédit EXACTEMENT la même action que l'expert.
   Mais dans ~10-15% des situations, il y a plusieurs actions valides (ex: ghost loin,
   plusieurs chemins vers la food). Le réseau peut choisir une action différente mais
   tout aussi bonne. C'est normal et ça ne veut pas dire que le modèle est mauvais.

   Q: Est-ce que le modèle va fonctionner sur Gradescope avec des mazes différents ?
   R: Oui ! Les features sont maze-aware :
   - legal_actions s'adapte automatiquement aux murs
   - dist_until_wall détecte les murs dans chaque direction
   - is_corner s'adapte à la topologie du maze
   Le modèle a appris des patterns généraux (fuir les ghosts, aller vers la food),
   pas juste à mémoriser un maze spécifique.
