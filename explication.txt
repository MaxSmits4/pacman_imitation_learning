PROJECT 2 - PACMAN IMITATION LEARNING - Documentation complète


PARTIE 1 : Vue d'ensemble

   But: apprendre à Pacman à imiter un expert en utilisant l'apprentissage supervisé. 
   Depuis un dataset de paires (GameState, action)
   et on veut entraîner un réseau de neurones à prédire quelle action l'expert aurait pris.

   Ordre de lecture : ici les 4 fichiers qu'on a modifié 
   1. data.py - Convertit GameState en 23 features
   2. architecture.py - Définit le réseau MLP (23 → 128 → 64 → 32 → 5)
   3. train.py - Entraîne le modèle
   4. pacmanagent.py - Utilise le modèle pour jouer


   Quick summary de chaque fichier :

   data.py (Feature Engineering)
   La partie la plus importante. On transforme le GameState en 23 features :
   position, ghosts, food, géométrie du labyrinthe, danger, actions légales.
   Tout est normalisé pour que le réseau converge bien.

   architecture.py (Réseau de neurones)
   MLP simple : 23 → 128 → 64 → 32 → 5 (3 couches cachées)
   Pattern par couche : Linear → BatchNorm → GELU → Dropout(0.15)

   train.py (Entraînement)
   Split train/validation 80/20, Adam optimizer, 50 epochs.
   On sauvegarde le meilleur modèle selon val_accuracy.

   pacmanagent.py (Agent)
   Utilise le modèle entraîné pour jouer.
   Convertit l'état → features → forward → softmax → meilleure action légale.


   Structure du projet :
   project2/
   ├── datasets/
   │   ├── pacman_dataset.pkl    → 15018 paires (state, action) de l'expert
   │   └── pacman_test.pkl       → États de jeu sans actions pour Gradescope
   ├── pacman_module/            → Moteur de jeu (NE PAS MODIFIER)
   ├── 1.data.py                 → Feature engineering + Dataset
   ├── 2.architecture.py         → Réseau de neurones
   ├── 3.train.py                → Boucle d'entraînement
   ├── 4.pacmanagent.py          → Agent qui joue
   ├── run.py                    → Visualisation du jeu
   ├── write_submission.py       → Génération du CSV
   ├── pacman_model.pth          → Poids du modèle entraîné
   └── submission.csv            → Prédictions pour Gradescope


PARTIE 2 : Data.py - Feature Engineering

   Un réseau de neurones ne peut pas apprendre directement d'un GameState brut.
   On doit extraire des informations pertinentes pour la prise de décision.

   Nos 23 features (toutes normalisées ~[0,1]):

   POSITION PACMAN (2 features):
   [0] px / 20       Position X de Pacman
   [1] py / 20       Position Y de Pacman

   INFORMATION GHOST (4 features):
   [2] dx_ghost / 20    Direction X vers le ghost le plus proche
   [3] dy_ghost / 20    Direction Y vers le ghost
   [4] ghost_dist / 20  Distance Manhattan au ghost
   [5] ghost_adjacent   1 si ghost à distance 1 (danger immédiat!)

   INFORMATION FOOD (4 features):
   [6] n_food / 50          Nombre de food restante
   [7] dx_food / 20         Direction X vers la food la plus proche
   [8] dy_food / 20         Direction Y vers la food
   [9] closest_food / 20    Distance à la food la plus proche

   GEOMETRIE DU LABYRINTHE (5 features):
   [10] dist_north / 10  Distance au mur au Nord
   [11] dist_south / 10  Distance au mur au Sud
   [12] dist_east / 10   Distance au mur à l'Est
   [13] dist_west / 10   Distance au mur à l'Ouest
   [14] is_corner        1 si Pacman est dans un coin (≤2 directions libres)

   FEATURES DE DANGER (3 features) - Les plus importantes selon moi:
   [15] danger_level      = 1/(ghost_dist), haut quand danger proche
   [16] ghost_blocks_food = 1 si ghost entre Pacman et la food
   [17] escape_options    = nombre de directions de fuite / 4

   ACTIONS LEGALES (5 features):
   [18] legal_north  1 si NORTH est légal
   [19] legal_south  1 si SOUTH est légal
   [20] legal_east   1 si EAST est légal
   [21] legal_west   1 si WEST est légal
   [22] legal_stop   1 si STOP est légal


   Comment on trouve la food la plus proche ?

   1. On récupère toutes les positions de food
      food_positions = [(5, 3), (7, 8), (2, 4), ...]

   2. On calcule la distance Manhattan à CHAQUE food
      distances = [abs(pac_x - food_x) + abs(pac_y - food_y)
                  for food_x, food_y in food_positions]

   3. On trouve laquelle est la plus proche
      closest_index = argmin(distances)

   4. On calcule la direction vers cette food
      direction_x = closest_food_x - pac_x
      direction_y = closest_food_y - pac_y

      Si direction_x > 0 → food à droite
      Si direction_x < 0 → food à gauche
      Si direction_y > 0 → food en haut
      Si direction_y < 0 → food en bas


   Géométrie du labyrinthe

   La fonction dist_until_wall() compte combien de cases Pacman peut avancer
   dans une direction avant de rencontrer un mur.

   Exemple visuel:
      # # # # # # #
      # . . . . . #
      # # P . . . #  ← Pacman en (2,2)
      # # # . . . #
      # # # # # # #

   dist_until_wall(2, 2, 1, 0) = 3  (peut aller 3 cases à droite)
   dist_until_wall(2, 2, -1, 0) = 0  (mur immédiat à gauche)

   Détection de coin :
   On compte combien de directions sont libres autour de Pacman.
   Si ≤2, c'est un coin (dangereux car peu d'options pour fuir).


PARTIE 3 : Architecture (architecture.py)

   Pourquoi un MLP et pas un CNN ?
   Les CNN sont faits pour les images 2D où les pixels voisins sont liés.
   Notre input est un vecteur (je dirais même un tenseur!) 1D de 23 features sans relation spatiale,
   donc un MLP est le bon choix.

   Structure : 23 → 128 → 64 → 32 → 5

   Pour chaque couche cachée ici 3: Linear → BatchNorm → GELU → Dropout

   1. Linear : transforme les données (multiplication matricielle + biais)
   2. BatchNorm : normalise les valeurs entre couches (stabilise l'entraînement)
   3. GELU : activation smooth, meilleure que ReLU, la meilleur que j'ai trouver, testez d'autre
   4. Dropout : prévient l'overfitting
   5. Pas d'activation finale : CrossEntropyLoss veut des logits bruts

   Pourquoi GELU et pas ReLU ?
   GELU est plus smooth (pas de coupure brutale à 0),
   meilleur flow de gradients, utilisé dans GPT et BERT.


PARTIE 4 : Entraînement (train.py)

   Pipeline :
   1. Charger le dataset (15018 échantillons)
   2. Split train/validation (80%/20%)
   3. Entraîner pendant 50 epochs
   4. Sauvegarder le meilleur modèle

   Hyperparamètres :
   - batch_size = 128  → Bon équilibre vitesse/stabilité
   - epochs = 50       → Assez pour converger
   - lr = 1e-3         → Learning rate standard pour Adam
   - val_ratio = 0.2   → 20% pour validation


   Loss function : CrossEntropyLoss

   La loss mesure à quel point le réseau se trompe (0 = parfait, plus c'est grand, plus il est loin de la bonne action).
   Le gradient, c'est la dérivée de cette loss par rapport aux poids : il indique dans quelle direction et de combien on doit modifier les poids pour réduire l'erreur (descente de gradient).

   Vu au cours pour les problèmes de classification.
   Mais pourquoi on a besoin d'une "loss" ? Explications depuis le début :

            ÉTAPE 1 : Qu'est-ce qu'on veut faire ?

            On veut que le réseau apprenne à imiter l'expert.
            Pour chaque situation de jeu, l'expert a choisi une action (ex: EAST).
            On veut que le réseau apprenne à faire le même choix.

            Exemple concret :
            - État du jeu : Pacman en (5,3), ghost à droite, food en haut
            - Expert a choisi : NORTH (indice 0)
            - Réseau prédit : [2.1, 0.5, 3.2, 1.0, 0.3]  ← nombres bruts (logits)
                              ↑    ↑    ↑    ↑    ↑
                              NORTH SOUTH EAST WEST STOP

            Problème : Le réseau prédit 3.2 pour EAST, mais l'expert voulait NORTH !
            On doit dire au réseau "tu t'es trompé, il fallait choisir NORTH".

            ÉTAPE 2 : Comment mesurer l'erreur ?

            D'abord, convertir les logits en probabilités avec SOFTMAX :

            Softmax(x_i) = exp(x_i) / Σ exp(x_j)

            Logits :        [2.1,  0.5,  3.2,  1.0,  0.3]
                              ↓     ↓     ↓     ↓     ↓
            Probabilités :  [0.18, 0.04, 0.54, 0.06, 0.03]
                              ↑
                           NORTH = 18% seulement (mais c'est la bonne action!)

            Les probabilités somment à 1.0 (= 100%).
            On voit que le réseau donne seulement 18% de confiance à NORTH,
            alors que c'est l'action correcte !

            ÉTAPE 3 : Calculer la perte (loss)

            La loss mesure "à quel point le réseau s'est trompé".

            On veut pénaliser le réseau quand :
            - La probabilité de la BONNE action est FAIBLE (mauvais !)
            - La probabilité de la BONNE action est ÉLEVÉE (bon !)

            Formule mathématique (pour 1 exemple) :
               Loss = -log(probabilité_de_la_bonne_action)

            Dans notre exemple :
               Bonne action = NORTH, probabilité = 0.18
               Loss = -log(0.18) = 1.71

            Plus la probabilité est basse, plus la loss est élevée (= grosse pénalité).
            Si probabilité = 1.0 (100% sûr), alors loss = 0 (parfait !).

            ÉTAPE 4 : Cross Entropy = moyenne sur tout le batch

            Un batch = groupe de 128 exemples traités ensemble.
            On calcule la loss pour CHAQUE exemple, puis on fait la moyenne.

            Formule complète (du cours) :
               L(θ) = -1/N Σ_(x_j,y_j)∈d Σ_{i=1}^C y_ij log f_i(x_j; θ)

            Où :
            - N = nombre d'exemples dans le batch (128)
            - C = nombre de classes (5 actions)
            - y_ij = 1 si l'exemple j appartient à la classe i, 0 sinon
            - f_i(x_j; θ) = probabilité prédite par le réseau pour la classe i

            En clair : Pour chaque exemple, prends -log(proba de la bonne action),
            puis fais la moyenne sur tout le batch.

            ÉTAPE 5 : Pourquoi CrossEntropyLoss en PyTorch ?

            Manuellement, il faudrait faire :
            1. Logits → Softmax → probabilités
            2. Prendre le log des probabilités → log-probabilités
            3. Sélectionner la bonne action et calculer -log(proba)
            4. Faire la moyenne

            PyTorch fait TOUT ça automatiquement avec CrossEntropyLoss() !

            Bonus : PyTorch utilise LogSoftmax au lieu de Softmax + Log séparément.
            Pourquoi ? Pour la stabilité numérique (éviter les overflows avec exp()).

            Résumé :
               CrossEntropyLoss = Softmax + Log + Sélection + Moyenne

            En une seule ligne de code, PyTorch :
            - Convertit les logits en probabilités
            - Calcule -log(probabilité de la bonne action)
            - Fait la moyenne sur le batch
            - Tout ça de manière stable et rapide !

            C'est pour ça qu'on n'a PAS besoin de mettre Softmax à la fin du réseau :
            CrossEntropyLoss s'attend à recevoir des logits bruts et fait le reste.

   Optimizer : Adam
   Adam est le choix par défaut pour le deep learning car il adapte
   le learning rate par paramètre et converge rapidement.

      Checkpointing :
      On sauvegarde le modèle avec la meilleure val_accuracy, pas le dernier,
      pour éviter de garder un modèle overfit.


PARTIE 5 : Agent (pacmanagent.py)

   Juste lire le .py suffit


PARTIE 6 : Questions possibles à l'oral - FAQ 

   Q: Pourquoi un MLP et pas un CNN?
   R: Les CNN sont faits pour les images 2D où les pixels voisins sont liés.
   Notre input est un vecteur 1D de 23 features sans relation spatiale,
   donc un MLP est le bon choix.

   Q: Pourquoi normaliser les features?
   R: Les NN convergent mieux quand les inputs sont dans la même échelle.
   Sans normalisation, certaines features domineraient les autres.

   Q: Pourquoi GELU?
   R: Activation smooth avec meilleur flow de gradients que ReLU.
   Utilisé dans GPT, BERT. Performance empiriquement supérieure.

   Q: Pourquoi BatchNorm?
   R: Sans BatchNorm, l'entraînement était instable.
   BatchNorm normalise les valeurs entre couches pour stabiliser les gradients.

   Q: Pourquoi CrossEntropyLoss?
   R: C'est la loss standard pour la classification multi-classe.
   Combine LogSoftmax et NLLLoss efficacement.

   Q: Pourquoi les features de danger?
   R: Le modèle avait du mal à éviter les ghosts avec juste la distance.
   danger_level donne un gradient de danger,
   ghost_blocks_food dit si le chemin vers la food est dangereux,
   escape_options dit combien de fuites sont possibles.

   Q: Comment gérer les actions illégales?
   R: On trie les prédictions par probabilité et on prend la première action
   qui est dans la liste des actions légales.

   Q: Pourquoi 50 epochs?
   R: Testé empiriquement. Moins = underfitting, plus = overfitting. Franchement pas besoin de plus. 


   Q: Pourquoi normaliser ?
   R: Les réseaux de neurones fonctionnent mieux quand toutes les features
   sont dans la même échelle.
   Sans normalisation, la position X pourrait être 0-20,
   le score pourrait être -500 à +1000, les flags sont 0 ou 1...
   Avec normalisation tout est ~[0,1], ce qui accélère la convergence
   et évite que certaines features dominent les autres.

   Q: Pourquoi convertir les actions en indices ?
   R: Les réseaux de neurones ne comprennent que les nombres,
   pas les objets Python comme Directions.NORTH.
   On crée donc deux mappings :

   ACTION_TO_INDEX = {
      Directions.NORTH: 0,
      Directions.SOUTH: 1,
      Directions.EAST: 2,
      Directions.WEST: 3,
      Directions.STOP: 4
   }

   INDEX_TO_ACTION = {0: Directions.NORTH, ...}

   Pendant l'entraînement :
   Expert joue EAST → converti en indice 2 → Le réseau apprend à prédire 2

   Pendant le jeu :
   Réseau prédit [0.05, 0.1, 0.75, 0.08, 0.02] → argmax = 2 → reconverti en EAST
